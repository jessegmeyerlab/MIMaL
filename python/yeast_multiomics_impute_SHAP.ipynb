{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import *\n",
    "\n",
    "# 1.3 Regressors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.inspection import permutation_importance\n",
    "import time\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE to Load saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklepath = 'C:\\\\Users\\\\qdickinson\\\\Jupyter Notebooks\\\\yeast_multiomics_impute_SHAP\\\\pickles\\\\'\n",
    "\n",
    "pro_train = pd.read_pickle(picklepath+'pro_train.pickle')\n",
    "#lip_train = pd.read_pickle(picklepath+'lip_train.pickle')\n",
    "met_train = pd.read_pickle(picklepath+'met_train.pickle')\n",
    "#ml_train = pd.read_pickle('ml_train.pickle')\n",
    "\n",
    "pro_val = pd.read_pickle(picklepath+'pro_val.pickle')\n",
    "#lip_val = pd.read_pickle(picklepath+'lip_val.pickle')\n",
    "met_val = pd.read_pickle(picklepath+'met_val.pickle')\n",
    "#ml_val = pd.read_pickle('ml_val.pickle')\n",
    "\n",
    "pro_test = pd.read_pickle(picklepath+'pro_test.pickle')\n",
    "#lip_test = pd.read_pickle(picklepath+'lip_test.pickle')\n",
    "met_test = pd.read_pickle(picklepath+'met_test.pickle')\n",
    "#ml_test = pd.read_pickle('ml_test.pickle')\n",
    "\n",
    "pro_train_all = pd.concat([pro_train, pro_val])\n",
    "#lip_train_all = pd.concat([lip_train, lip_val])\n",
    "met_train_all = pd.concat([met_train, met_val])\n",
    "#ml_train_all = pd.read_pickle('ml_train_all.pickle')\n",
    "\n",
    "\n",
    "pro_train_clean_names = pro_train\n",
    "pro_train_clean_names.columns = [name.split(' ')[0] for name in pro_train_clean_names.columns.values.tolist()]\n",
    "pro_test_clean_names = pro_test\n",
    "pro_test_clean_names.columns = [name.split(' ')[0] for name in pro_test_clean_names.columns.values.tolist()]\n",
    "\n",
    "\n",
    "#create met_all, remove duplicate glyceric acid\n",
    "\n",
    "met_all = pd.concat([met_test,met_val,met_train])\n",
    "column_numbers = [x for x in range(met_all.shape[1])]  # list of columns' integer indices\n",
    "column_numbers.remove(30) #removing column integer index 0\n",
    "met_all = met_all.iloc[:, column_numbers] #return all columns except the 0th column\n",
    "\n",
    "column_names = met_all.columns\n",
    "\n",
    "#create pro_all\n",
    "\n",
    "pro_all = pd.concat([pro_test,pro_val,pro_train])\n",
    "\n",
    "pro_all_clean_names = pro_all\n",
    "pro_all_clean_names.columns = [name.split(' ')[0] for name in pro_all_clean_names.columns.values.tolist()]\n",
    "\n",
    "protein = pro_train_all.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, model_name, XTRAIN, XTEST,YTRAIN, YTEST):\n",
    "    model.fit(XTRAIN, YTRAIN)\n",
    "    YPRED = model.predict(XTEST)\n",
    "    print('Mean squared error: %.4f'\n",
    "      % mean_squared_error(YTEST, YPRED))\n",
    "    # The coefficient of determination: 1 is perfect prediction\n",
    "    print('Coefficient of determination: %.4f'\n",
    "      % r2_score(YTEST, YPRED))\n",
    "    # Plot outputs\n",
    "    plt.rcParams['figure.figsize'] = 5,5\n",
    "    #sns.jointplot(YTEST, YPRED,  color='black', kind='reg')\n",
    "    plt.scatter(YTEST, YPRED, color='blue')\n",
    "    plt.xlabel('true')\n",
    "    plt.ylabel('pred')\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.suptitle(model_name, fontsize = 16)\n",
    "    plt.show()\n",
    "    return mean_squared_error(YTEST, YPRED), r2_score(YTEST, YPRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try many models\n",
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "model_dict = {\n",
    "    'dummy': DummyRegressor(),\n",
    "    'linear': LinearRegression(),\n",
    "    'lasso' : Lasso(),\n",
    "    'elastic': ElasticNet(),\n",
    "    'ridge': Ridge(),\n",
    "    'Sup Vec Regr': MultiOutputRegressor(SVR()),\n",
    "    'M/O adaB': MultiOutputRegressor(AdaBoostRegressor()),\n",
    "    'M/O gboost': MultiOutputRegressor(GradientBoostingRegressor()) , \n",
    "    'ExtraTrees': ExtraTreesRegressor(),\n",
    "    'RandomForest' : RandomForestRegressor(),\n",
    "    \"XGboost\" : XGBRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loop through all metabolites - look for rho vs importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### for metabolites\n",
    "tables_dict_met = {}\n",
    "mse_dict_met = {}\n",
    "r2_dict_met = {}\n",
    "met_model_dict = {}\n",
    "met_permimp_dict={}\n",
    "for i in range(0, len(met_train_all.columns)):\n",
    "    print(i)\n",
    "    print(met_train_all.columns[i])\n",
    "    save_name = met_test.columns[i].replace('(','').replace(')','').replace(':', '.').replace('/','_')\n",
    "    etr = ExtraTreesRegressor(n_estimators=500, max_depth=50,random_state=42, n_jobs=-1)\n",
    "    etr.fit(pro_train_all, np.asarray(met_train_all)[:,i])\n",
    "    met_model_dict[met_train_all.columns[i]] = etr\n",
    "    \n",
    "    ## metrics\n",
    "    y_pred = etr.predict(pro_test)\n",
    "    tmp_mse = np.round(mean_squared_error(np.asarray(met_test)[:,i],y_pred), 5)##########\n",
    "    tmp_r2 = np.round(r2_score(np.asarray(met_test)[:,i], y_pred), 5)###########\n",
    "    mse_dict_met[met_train_all.columns[i]] = tmp_mse ############\n",
    "    r2_dict_met[met_train_all.columns[i]] = tmp_r2 ############\n",
    "    print('r2=', r2_dict_met[met_train_all.columns[i]])\n",
    "\n",
    "    # plot correlation\n",
    "    lim = 0.1 + np.abs([y_pred, np.asarray(met_test)[:,i]]).max()\n",
    "    g = sns.jointplot(np.asarray(met_test)[:,i],y_pred,  xlim=[-lim,lim],\n",
    "                  ylim=[-lim, lim], kind='reg').set_axis_labels('measured', 'predicted', FONTSIZE=24)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(met_test.columns[i] +', MSE='+\n",
    "                   str(np.round(mean_squared_error(np.asarray(met_test)[:,i],y_pred), 6)), \n",
    "                   fontsize=24)\n",
    "    plt.text(-lim+(lim*0.1), lim-lim*0.2, '$r^2$= '+str(tmp_r2)) ############\n",
    "    plt.savefig('plots_imp\\\\pmet\\\\'+met_test.columns[i]+'_'+str(i)+'.svg')\n",
    "    plt.savefig('plots_imp\\\\pmet\\\\'+met_test.columns[i]+'_'+str(i)+'.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # make table with feature importances\n",
    "    feature_importances = pd.DataFrame(etr.feature_importances_,\n",
    "                                       index = protein.T.columns,\n",
    "                                        columns=['importance'])\n",
    "    \n",
    "    #get Permutation Importances\n",
    "    print('starting permutation importance', i)\n",
    "    start = time.time()\n",
    "    met_permimp_dict[met_train_all.columns[i]] = permutation_importance(etr, pro_test, \n",
    "                                          met_test.iloc[:,i], n_repeats=5, random_state=42, n_jobs=-1)   \n",
    "    print('computation done for '+str(met_train_all.columns[i]))\n",
    "    print('seconds = ', time.time() - start)\n",
    "    print('=============================')\n",
    "    feature_importances['perm_imp'] = met_permimp_dict[met_train_all.columns[i]].importances_mean\n",
    "    \n",
    "    ## get correlation between all protein profiles and the metabolite\n",
    "    spearman_rho = []\n",
    "    for prot_i in range(0, len(pro_val.columns) ):\n",
    "        tmp_rho, tmp_p = spearmanr(np.asarray(met_train_all)[:,i], np.asarray(pro_train_all)[:,prot_i])\n",
    "        spearman_rho.append(tmp_rho)\n",
    "    spearman_rhos = pd.DataFrame(spearman_rho,\n",
    "                                       index = protein.T.columns,\n",
    "                                        columns=['rho'])\n",
    "    feature_importances['rho'] = spearman_rhos\n",
    "    feature_importances['log10(importance)'] = np.log10(feature_importances['importance'])\n",
    "    feature_importances.sort_values('rho', ascending=False)\n",
    "    \n",
    "    # record table to dictionary, and save as CSV\n",
    "    tables_dict_met[met_val.columns[i]] = feature_importances\n",
    "    feature_importances.to_csv('tables_imp\\\\pmet\\\\'+save_name+'_num'+str(i)+'.csv')\n",
    "    \n",
    "    # plot the correlation rho versus the protein value\n",
    "    sns.set_style('white')\n",
    "    g = sns.jointplot(np.log10(feature_importances['importance']), spearman_rho)\n",
    "    sns.set(font_scale=2)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(met_train.columns[i] , fontsize=24)\n",
    "    g.set_axis_labels('log10(Gini Importance)', 'Spearman rho', fontsize=24)\n",
    "    plt.savefig('plots_imp\\\\pmet\\\\'+ save_name+ '_rhoVsGini_num'+str(i)+'.svg')\n",
    "    plt.close()\n",
    "    \n",
    "    ## plot the correlation rho verus mean perm importance\n",
    "    sns.set_style('white')\n",
    "    add_small_num = abs(np.min(feature_importances['perm_imp']))+0.00001\n",
    "    print(add_small_num)\n",
    "    g = sns.jointplot(np.log10(feature_importances['perm_imp']+add_small_num), spearman_rho)\n",
    "    sns.set(font_scale=2)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(met_train.columns[i] , fontsize=24)\n",
    "    g.set_axis_labels('Permutation Importance', 'Spearman rho', fontsize=24)\n",
    "    plt.savefig('plots_imp/pmet/'+ save_name + '_rhoVsPerm_num'+str(i)+'.svg')\n",
    "    plt.close()\n",
    "    \n",
    "pickle.dump(met_permimp_dict, open('pickle_imp\\\\ETmet_permimp_dict.pickle', 'wb'))\n",
    "pickle.dump(tables_dict_met, open('pickle_imp\\\\ETmet_tables_dict.pickle', 'wb'))\n",
    "pickle.dump(mse_dict_met, open('pickle_imp\\\\ETmet_mse_dict.pickle', 'wb'))\n",
    "pickle.dump(r2_dict_met, open('pickle_imp\\\\ETmet_r2_dict.pickle', 'wb'))\n",
    "pickle.dump(met_model_dict, open('pickle_imp\\\\ETmet_model_dict.pickle', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Saved Model\n",
    "\n",
    "met_model_dict = pickle.load(open(picklepath+'ETmet_model_dict_imp.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_clean(MODEL_DICT, SAVE_NAME,TESTDF, PLOTNUM, YPRED):\n",
    "    sns.reset_defaults()\n",
    "    sns.set_style('white')\n",
    "    plt.rcParams['figure.figsize'] = 3,3\n",
    "    y_pred = MODEL_DICT[TESTDF.columns[PLOTNUM]].predict(pro_test)\n",
    "    #print(TESTDF.iloc[:,PLOTNUM].values)\n",
    "    tmpdf = pd.DataFrame({'true':TESTDF.iloc[:,PLOTNUM].values, 'pred':y_pred})\n",
    "    #rint(tmpdf)\n",
    "    #print(y_pred)\n",
    "    g = sns.regplot('true', 'pred', data = tmpdf, color='black')\n",
    "    xmin, xmax = plt.xlim()\n",
    "    xrange = xmax-xmin\n",
    "    ymin, ymax = plt.ylim()\n",
    "    yr = ymax-ymin\n",
    "    plt.xlim(xmin+(xmin*0.1), xmax+xmax*0.15)\n",
    "    plt.ylim(ymin+(ymin*0.1), ymax+(ymax*0.15) )\n",
    "    plt.xlabel('measured')\n",
    "    plt.ylabel('predicted')\n",
    "    #g.axes.xaxis.set_label_position(\"top\")\n",
    "    #g.axes.xaxis.set_ticks_position(\"top\")\n",
    "    #g.axes.xaxis.set_ticks_position(\"top\")\n",
    "\n",
    "    plt.text(xmin+xrange*0.05, ymax-yr*0.05, TESTDF.columns[PLOTNUM])\n",
    "    plt.text(xmin+xrange*0.05, ymax-yr*0.14, '$R^2$= '+str(np.round(r2_dict_lip[TESTDF.columns[PLOTNUM]],3) ))\n",
    "    plt.text(xmin+xrange*0.05, ymax-yr*0.23, 'MSE= '+str(np.round(mse_dict_lip[TESTDF.columns[PLOTNUM]], 3) ))\n",
    "    #g.set(yticklabels=[-1,'',0,1,2,3])\n",
    "    #plt.yticks([-1,0,1,2,3])\n",
    "    plt.savefig(SAVE_NAME +'_regression.svg', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now metabolites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_clean2(MODEL_DICT, SAVE_DIR, SAVE_NAME,TESTDF, PLOTNUM, YPRED):\n",
    "    sns.reset_defaults()\n",
    "    sns.set_style('white')\n",
    "    plt.rcParams['figure.figsize'] = 3,3\n",
    "    y_pred = MODEL_DICT[SAVE_NAME].predict(pro_test)\n",
    "    #print(TESTDF.iloc[:,PLOTNUM].values)\n",
    "    tmpdf = pd.DataFrame({'true':TESTDF.iloc[:,PLOTNUM].values, 'pred':y_pred})\n",
    "    #rint(tmpdf)\n",
    "    #print(y_pred)\n",
    "    g = sns.regplot('true', 'pred', data = tmpdf, color='black')\n",
    "    xmin, xmax = plt.xlim()\n",
    "    xrange = xmax-xmin\n",
    "    ymin, ymax = plt.ylim()\n",
    "    yr = ymax-ymin\n",
    "    plt.xlim(xmin+(xmin*0.1), xmax+xmax*0.15)\n",
    "    plt.ylim(ymin+(ymin*0.1), ymax+(ymax*0.15) )\n",
    "    plt.xlabel('measured')\n",
    "    plt.ylabel('predicted')\n",
    "    #g.axes.xaxis.set_label_position(\"top\")\n",
    "    #g.axes.xaxis.set_ticks_position(\"top\")\n",
    "    #g.axes.xaxis.set_ticks_position(\"top\")\n",
    "\n",
    "    plt.text(xmin+xrange*0.05, ymax-yr*0.05, TESTDF.columns[PLOTNUM])\n",
    "    plt.text(xmin+xrange*0.05, ymax-yr*0.14, '$R^2$= '+str(np.round(r2_dict_met[SAVE_NAME],3) ))\n",
    "    plt.text(xmin+xrange*0.05, ymax-yr*0.23, 'MSE= '+str(np.round(mse_dict_met[SAVE_NAME], 3) ))\n",
    "    #g.set(yticklabels=[-1,'',0,1,2,3])\n",
    "    #plt.yticks([-1,0,1,2,3])\n",
    "    plt.savefig(SAVE_DIR+ SAVE_NAME +'_regression.svg', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ALL SHAPS\n",
    "\n",
    "shap_dictall = {}\n",
    "\n",
    "explainer_dictall = {}\n",
    "for k in range(len(met_all.columns)):\n",
    "    print(met_all.columns[k])\n",
    "    key = met_all.columns[k].replace('(','').replace(')','').replace(':', '.').replace('/','_').replace(' ','-').replace('?','')\n",
    "    explainer = shap.TreeExplainer(met_model_dict[key])\n",
    "    explainer_dictall[met_all.columns[k]] = explainer\n",
    "    shap_values = explainer.shap_values(pro_all)\n",
    "    shap_dictall[met_all.columns[k]] = shap_values\n",
    "    \n",
    "    #print(shap_values)\n",
    "    # summarize the effects of all the features\n",
    "    shap.summary_plot(shap_values, pro_all_clean_names, show=False, title=str(met_all.columns[k]) )\n",
    "    #plt.title(met_test.columns[k])\n",
    "    plt.savefig('pickle_imp/'+key+'_all.svg', bbox_inches=\"tight\")\n",
    "    plt.savefig('pickle_imp/'+key+'_all.png', bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "pickle.dump(explainer_dictall, open('pickle_imp/shap_pmet_all.pickle', 'wb'))    \n",
    "pickle.dump(shap_dictall, open('pickle_imp/shap_pmet_all.pickle', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_dictall = pickle.load(open(picklepath + \"explainer_pmet_all.pickle\", 'rb')) \n",
    "shap_dictall = pickle.load(open(picklepath+'shap_pmet_all.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values=shap_dictall[met_all.columns[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, pro_all_clean_names, show=False, title='citrate' )\n",
    "#plt.savefig('plots2/pmet/shap/shap_summary_citrate.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer_dictall[met_all.columns[20]].expected_value, shap_values[20,:], pro_test_clean_names.iloc[20,:], \n",
    "                show=True, \n",
    "                matplotlib=True,\n",
    "               text_rotation=90)\n",
    "#plt.savefig('plots2/pmet/shap/forceplot_'+met_test.columns[k]+str(i)+'.svg')\n",
    "#plt.savefig('plots2/pmet/shap/forceplot_'+met_test.columns[k]+str(i)+'.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 3,3\n",
    "y_pred = met_model_dict['Citric-acid'].predict(pro_test)\n",
    "g= sns.regplot(met_test.iloc[:,2], y_pred, color='black')\n",
    "plt.xlim(-1, 3)\n",
    "plt.ylim(-1, 3)\n",
    "plt.xlabel('measured')\n",
    "plt.ylabel('predicted')\n",
    "g.axes.xaxis.set_label_position(\"top\")\n",
    "g.axes.xaxis.set_ticks_position(\"top\")\n",
    "g.axes.xaxis.set_ticks_position(\"top\")\n",
    "\n",
    "plt.text(-0.8, 2.5, \"Citric Acid\")\n",
    "plt.text(-0.8, 2.1, '$r^2$= '+str(r2_dict_met[met_all.columns[20]]) )\n",
    "plt.text(-0.8, 1.7, 'MSE= '+str(mse_dict_met[met_all.columns[20]]) )\n",
    "\n",
    "#g.set(yticklabels=[-1,'',0,1,2,3])\n",
    "plt.yticks([-1,0,1,2,3])\n",
    "\n",
    "#plt.savefig('plots2/pmet/clean_citric_acid_regression.svg', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
